{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, urllib.request\n",
    "import xmltodict\n",
    "import json\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Overview**\n",
    "\n",
    "Personally, I am a person whom enjoys the research community and the new methods created and applied by researchers and developers around the world. When I am in the mood, I will often go to [arXiv](https://arxiv.org), an open access archive created and maintained by Cornell University for scholarly articles in the sciences, to read up on interesting new methods created and or applied within Computer Science, Statistics, Mathematics, and Economics. arXiv is vast, with nearly two million documents in their database. Thankfully they not only have a search engine for finding new papers, but also an API for retrieving scholarly articles within various categories according to their [taxonomy](https://arxiv.org/category_taxonomy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src=\"/Users/wastechs/Documents/git-repos/arxiv_mongo/images/arxiv.png\" width=\"512\" height=\"174\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, I am interested in the taxonomies of:\n",
    "\n",
    "**Computer Science**\n",
    " - cs.AI = Artificial Intelligence\n",
    " - cs.CE = Computational Engineering\n",
    " - cs.DB = Databases\n",
    " - cs.ET = Emerging Technologies\n",
    " - cs.DC = Distributed Computing\n",
    " - cs.LG = Machine Learning\n",
    " - cs.IT = Information Theory\n",
    "\n",
    "**Statistics**\n",
    " - stat.AP = Statistical Applications\n",
    " - stat.ML = Machine Learning\n",
    " - stat.TH = Theory\n",
    " - stat.ME = Methodology\n",
    "\n",
    "**Mathematics**\n",
    " - math.PR = Probability Theory\n",
    " - math.ST = Mathematical Statistics\n",
    "\n",
    "**Economics**\n",
    " - econ.EM = Econometrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to not only learn about MongoDB, but to also inform myself of relevant research articles and researchers in the areas I am interested in. However, due to the limiting factor of storage constraints on the free tier of MongoDB Atlas, only the top 1000 documents, **by relevance**, for each category were retrieved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ETL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Information\n",
    "cnx = 'mongodb+srv://gabe:gabe_mongo@arxiv.xawxi.mongodb.net/test'\n",
    "# Connection to MongoDB\n",
    "client = pymongo.MongoClient(cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access 'arXiv' database\n",
    "db = client['arxiv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fetch Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arXiv category taxonomy for the \"for loop\"\n",
    "csCats = ['cs.AI', 'cs.CE', 'cs.DB', 'cs.ET', 'cs.DC', 'cs.LG', 'cs.IT']\n",
    "statCats = ['stat.AP', 'stat.ML', 'stat.TH', 'stat.ME']\n",
    "mathCats = ['math.PR', 'math.ST']\n",
    "econCats = ['econ.EM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arxiv(db, collection, category=list, file=bool):\n",
    "\n",
    "    if collection == 'Math':\n",
    "        col = db.Math\n",
    "    elif collection == 'ComputerScience':\n",
    "        col = db.ComputerScience\n",
    "    elif collection == 'Economics':\n",
    "        col = db.Economics\n",
    "    elif collection == 'Statistics':\n",
    "        col = db.Statistics\n",
    "    else:\n",
    "        raise ValueError('Collection not in MongoDB')\n",
    "\n",
    "    if type(category) != list:\n",
    "        raise TypeError('Category is not in list format')\n",
    "    else:\n",
    "        for cat in category:\n",
    "            url = 'http://export.arxiv.org/api/query?search_query=cat:{}&start=0&max_results=1000&sortBy=relevance&sortOrder=ascending'.format(cat)\n",
    "            data = urllib.request.urlopen(url)\n",
    "            arxiv_data = data.read().decode('utf-8')\n",
    "\n",
    "            # Returned data is an \"Atom Document\" - convert to ordered dictionary\n",
    "            arxiv_dict = xmltodict.parse(arxiv_data)\n",
    "\n",
    "            # Converting to JSON\n",
    "            arxivJSON = json.dumps(arxiv_dict, indent=4)\n",
    "\n",
    "            # Decoding JSON\n",
    "            arxiv_final = json.loads(arxivJSON)\n",
    "\n",
    "            # Insert document into collection\n",
    "            try:\n",
    "                col.insert_many(arxiv_final['feed']['entry'])\n",
    "                print('Document {} inserted into {} collection'.format(cat, collection))\n",
    "            except:\n",
    "                print('An error has occured')\n",
    "\n",
    "            # Optional - write and save to file\n",
    "            if file == True:\n",
    "                with open('{}.json'.format(cat.replace('.', '_')), 'w') as write_file:\n",
    "                    json.dump(arxiv_dict, write_file, indent=4)\n",
    "\n",
    "    \n",
    "    return arxiv_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#math = get_arxiv(db, 'Math', mathCats, False)\n",
    "#cs = get_arxiv(db, 'ComputerScience', csCats, False)\n",
    "#stat = get_arxiv(db, 'Statistics', statCats, False)\n",
    "#econ = get_arxiv(db, 'Economics', econCats, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Document Structure**\n",
    "\n",
    "The documents returned by the arXiv API were converted to JSON format and straight away imported into their respective collection in the arXiv database. The class diagram below represents what a _single collection_ looks like. However, all collections have the same structure. The '{}' indicates a nested substructure where the additional data is found linked below the main document (publication). A particularity worth noting is that the '@' in the nested substructure is a formatting design by arXiv where, as will be seen below in the analysis, one can simply use dot notation for the field to access the information, i.e,. 'arxiv:journal_ref.@xmlns:arxiv'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![class diagram](/Users/wastechs/Documents/git-repos/arxiv_mongo/images/class_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis**\n",
    "\n",
    "In performing analysis, the following system architecture was developed to gain a more intuitive understanding of how the Mongo aggregation pipelines were utilized. Starting with the analyst, they develop aggregation queries in their IDE or text editor of choice which are then sent to the arXiv database using the Python library _pymongo_. From there, the queries are processed on the MongoDB server and the results are returned back to the client which are then displayed to the analyst. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"/Users/wastechs/Documents/git-repos/arxiv_mongo/images/system-arch-flow-3.png\" width=\"1058\" height=\"554\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The queries designed here sometimes reflect my personal interests. For example, in regard to statistics, I classify myself as a Bayesian and thus, I have constructured a regex pipeline for finding any document containing Bayes, Bayesian, Bayesianism. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all of the collections in the MongoDB\n",
    "db = client['arxiv']\n",
    "collections = db.list_collection_names()\n",
    "collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents in each collection\n",
    "db.Math.count_documents({}), db.ComputerScience.count_documents({}), db.Economics.count_documents({}), db.Statistics.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authors with the most relevant papers in Statistics\n",
    "project = {'$project': {'_id': 0, 'author.name':1}}\n",
    "unwind = {'$unwind': '$author.name'}\n",
    "groupby = {'$group': {'_id': '$author.name', 'count': {'$sum': 1}}}\n",
    "\n",
    "pipeline = [project, unwind, groupby]\n",
    "\n",
    "statAuthors = db.Statistics.aggregate(pipeline)\n",
    "\n",
    "statAuthors = pd.DataFrame(statAuthors)\n",
    "statAuthors.sort_values(by=['count'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of my favorite machine learning researchers at the moment - Does he have any relevant papers?\n",
    "for doc in db.ComputerScience.aggregate([\n",
    "    {'$match': {'author.name': 'Kilian Q. Weinberger'}},\n",
    "    {'$project': {'title': 1, 'author.name': 1, '_id': 0}}]):\n",
    "\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics papers with \"Baye\" in the title\n",
    "for doc in db.Statistics.aggregate([\n",
    "    {'$project': {'_id': 0,\n",
    "                  'title': 1,\n",
    "                  'author.name': 1}},\n",
    "    {'$match': {'title': {'$regex': '^Bayes'}}}\n",
    "]):\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematics and Computer Science complement each other very well\n",
    "stage_lookup = {\n",
    "    '$lookup': {\n",
    "        'from': 'Math',\n",
    "        'localField': 'author.name',\n",
    "        'foreignField': 'author.name',\n",
    "        'as': 'same_author'\n",
    "    }\n",
    "}\n",
    "\n",
    "match = {'$match': {'same_author.0': {'$exists': True}}}\n",
    "\n",
    "add_fields = {'$addFields': {\n",
    "    'author_name': 'author.name',\n",
    "    'paper_title': 'title'\n",
    "}}\n",
    "\n",
    "project = {'$project': {'_id': 0, 'author.name':1, 'title': 1}}\n",
    "\n",
    "unwind = {'$unwind': '$author.name'}\n",
    "\n",
    "group_by = {'$group': {'_id': '$author.name', 'count': {'$sum': 1}}}\n",
    "\n",
    "limit = {'$limit': 3}\n",
    "\n",
    "pipeline = [stage_lookup, match, project, add_fields, project, limit]\n",
    "#pipeline = [stage_lookup, match, project, unwind, group_by, limit]\n",
    "\n",
    "for doc in db.ComputerScience.aggregate(pipeline):\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the advancements in Computer Science and AI the past decade(s) I wonder how many revelant papers\n",
    "# are in the more \"recent\" years?\n",
    "def art_by_year(col, cat):\n",
    "    project = {'$project': {'_id': 0}}\n",
    "    group_by = {'$group': {'_id': {'year': {'$year': '$formatted_date'}},\n",
    "                'count': {'$sum': 1}}}\n",
    "    \n",
    "    group_by_date = col.aggregate([project, group_by])\n",
    "    byYear = pd.DataFrame(group_by_date)\n",
    "    byYear['_id'] = pd.json_normalize(byYear['_id'])\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.barplot(x='_id', y='count', data=byYear)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel('Year')\n",
    "    plt.title('{} Articles'.format(cat))\n",
    "    plt.show()\n",
    "\n",
    "    return byYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csYear = art_by_year(db.ComputerScience_Clean, 'CS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6ea8e6696542bf91b5850513673ceef1808937d627ee3e2c71e2007cafbd44d0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('autodidact': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
